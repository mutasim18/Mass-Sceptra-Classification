{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WOrking on CM1 dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   #Point  X(Thompsons)    Y(Counts)\n",
      "0       0          40.0    46.235340\n",
      "1       1          40.9    58.811787\n",
      "2       2          41.0   617.287781\n",
      "3       3          41.2    62.857056\n",
      "4       4          42.1  3057.952637\n",
      "Rows & columns: (92310, 3)\n"
     ]
    }
   ],
   "source": [
    "# Loading CM1 folder\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Loading folder path\n",
    "CM1_path = r\"C:\\Local Disk (A)\\Github Files\\Projects\\Mass-Sceptra-Classification\\Datasets\\CM1\"\n",
    "data = []\n",
    "\n",
    "# Combining all the files in the CM1 folder\n",
    "for file in os.listdir(CM1_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(CM1_path, file)\n",
    "        df = pd.read_csv(file_path, skiprows=1) # skipping the first row\n",
    "        # Append to the list of DataFrames\n",
    "        data.append(df)\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_CM1_data = pd.concat(data, ignore_index=True)\n",
    "\n",
    "print(combined_CM1_data.head(5))\n",
    "print(\"Rows & columns:\", combined_CM1_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary :\n",
      "             #Point  X(Thompsons)      Y(Counts)\n",
      "count  92310.000000  92310.000000   92310.000000\n",
      "mean     171.147308    143.243818    1193.070302\n",
      "std      107.140743     68.833009    9722.745794\n",
      "min        0.000000     39.900000       0.003136\n",
      "25%       82.000000     88.200000      22.855255\n",
      "50%      164.000000    134.000000      70.941631\n",
      "75%      248.000000    186.100000     283.324089\n",
      "max      521.000000    413.200000  654898.562500\n",
      "\n",
      "Cheking for missing values :\n",
      "#Point          0\n",
      "X(Thompsons)    0\n",
      "Y(Counts)       0\n",
      "dtype: int64\n",
      "\n",
      "After droping '#Point' column :\n",
      "   X(Thompsons)    Y(Counts)\n",
      "0          40.0    46.235340\n",
      "1          40.9    58.811787\n",
      "2          41.0   617.287781\n",
      "3          41.2    62.857056\n",
      "4          42.1  3057.952637\n",
      "\n",
      "After Rounding :\n",
      "   X(Thompsons)    Y(Counts)\n",
      "0          40.0    46.235340\n",
      "1          41.0    58.811787\n",
      "2          41.0   617.287781\n",
      "3          42.0    62.857056\n",
      "4          43.0  3057.952637\n",
      "\n",
      "Creating common m/z axis and aligning spectra...\n",
      "\n",
      "Calculating Cosine Similarity...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (92310,) and (1000,) not aligned: 92310 (dim 0) != 1000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Adding new feature: Cosine Similarity between original and interpolated values\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCalculating Cosine Similarity...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m cosine_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m cosine(combined_CM1_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY(Counts)\u001b[39m\u001b[38;5;124m'\u001b[39m], interpolated_y)\n\u001b[0;32m     43\u001b[0m combined_CM1_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cosine_similarity\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Adding new feature: Area Ratio using trapezoidal rule to calculate the area under the curve\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Software\\Anaconda\\File\\Lib\\site-packages\\scipy\\spatial\\distance.py:694\u001b[0m, in \u001b[0;36mcosine\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;124;03mCompute the Cosine distance between 1-D arrays.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    690\u001b[0m \n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;66;03m# cosine distance is also referred to as 'uncentered correlation',\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;66;03m#   or 'reflective correlation'\u001b[39;00m\n\u001b[1;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m correlation(u, v, w\u001b[38;5;241m=\u001b[39mw, centered\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Software\\Anaconda\\File\\Lib\\site-packages\\scipy\\spatial\\distance.py:644\u001b[0m, in \u001b[0;36mcorrelation\u001b[1;34m(u, v, w, centered)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m     vw, uw \u001b[38;5;241m=\u001b[39m v, u\n\u001b[1;32m--> 644\u001b[0m uv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(u, vw)\n\u001b[0;32m    645\u001b[0m uu \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(u, uw)\n\u001b[0;32m    646\u001b[0m vv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(v, vw)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (92310,) and (1000,) not aligned: 92310 (dim 0) != 1000 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Preprocessing Data\n",
    "\n",
    "# summary of the datset\n",
    "print(\"Summary :\")\n",
    "summary = combined_CM1_data.describe()\n",
    "print(summary)\n",
    "\n",
    "# Cheking for missing values\n",
    "print(\"\")\n",
    "print(\"Cheking for missing values :\")\n",
    "is_null = combined_CM1_data.isnull().sum()\n",
    "print(is_null)\n",
    "\n",
    "# dropping \"#Point\" column as it is not relevant\n",
    "print(\"\")\n",
    "print(\"After droping '#Point' column :\")\n",
    "combined_CM1_data = combined_CM1_data.drop('#Point', axis= 1)\n",
    "print(combined_CM1_data.head(5))\n",
    "\n",
    "# Rounding up the values of X(Thompsons) column\n",
    "print(\"\")\n",
    "print(\"After Rounding :\")\n",
    "import numpy as np\n",
    "combined_CM1_data['X(Thompsons)'] = np.ceil(combined_CM1_data['X(Thompsons)'])\n",
    "print(combined_CM1_data.head(5))\n",
    "\n",
    "# Adding features\n",
    "\n",
    "# Import necessary libraries for feature calculations\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Adding features\n",
    "print(\"\\nCreating new features...\")\n",
    "\n",
    "# Define common m/z axis points to align the data\n",
    "common_mz = np.linspace(combined_CM1_data['X(Thompsons)'].min(), combined_CM1_data['X(Thompsons)'].max(), 1000)\n",
    "\n",
    "# Create empty lists to store feature values for each row\n",
    "cosine_similarities = []\n",
    "area_ratios = []\n",
    "std_ratios = []\n",
    "\n",
    "# Iterate through each row in the dataset\n",
    "for i in range(len(combined_CM1_data)):\n",
    "    # Extract the current row values\n",
    "    x_val = combined_CM1_data.loc[i, 'X(Thompsons)']\n",
    "    y_val = combined_CM1_data.loc[i, 'Y(Counts)']\n",
    "\n",
    "    # Align current row's Y(Counts) onto the common m/z axis using interpolation\n",
    "    interpolated_y = np.interp(common_mz, [x_val], [y_val])\n",
    "\n",
    "    # Calculate cosine similarity (since only one point is interpolated, cosine similarity will be 1)\n",
    "    cosine_similarities.append(1.0)\n",
    "\n",
    "    # Calculate area ratio (area will be the same as the only point is aligned)\n",
    "    area_original = y_val\n",
    "    area_interpolated = interpolated_y[0]\n",
    "    area_ratios.append(min(area_original / area_interpolated, area_interpolated / area_original) if area_original > 0 and area_interpolated > 0 else 0)\n",
    "\n",
    "    # Calculate standard deviation ratio (since it's a single value, ratio is 1)\n",
    "    std_ratios.append(1.0)\n",
    "\n",
    "# Add the new features to the DataFrame\n",
    "combined_CM1_data['cosine_similarity'] = cosine_similarities\n",
    "combined_CM1_data['area_ratio'] = area_ratios\n",
    "combined_CM1_data['std_ratio'] = std_ratios\n",
    "\n",
    "# Display the DataFrame with the new features\n",
    "print(\"\\nAfter adding new features:\")\n",
    "print(combined_CM1_data.head())\n",
    "\n",
    "# Normalizing the dataset\n",
    "\"\"\"print(\"\")\n",
    "print(\"After Normalization :\")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_normalize = ['X(Thompsons)', 'Y(Counts)']\n",
    "combined_CM1_data[columns_to_normalize] = scaler.fit_transform(combined_CM1_data[columns_to_normalize])\n",
    "print(combined_CM1_data.head(5))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(boosting_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbdt\u001b[39m\u001b[38;5;124m'\u001b[39m, num_leaves\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Software\\Anaconda\\File\\Lib\\site-packages\\lightgbm\\sklearn.py:1237\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m   1236\u001b[0m _LGBMAssertAllFinite(y)\n\u001b[1;32m-> 1237\u001b[0m _LGBMCheckClassificationTargets(y)\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le \u001b[38;5;241m=\u001b[39m _LGBMLabelEncoder()\u001b[38;5;241m.\u001b[39mfit(y)\n\u001b[0;32m   1239\u001b[0m _y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(y)\n",
      "File \u001b[1;32mc:\\Software\\Anaconda\\File\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:221\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    213\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m ]:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split dataset into features and target\n",
    "X = combined_CM1_data[['X(Thompsons)']]  # Features: X(Thompsons)\n",
    "y = combined_CM1_data['Y(Counts)']  # Target: Y(Counts)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n",
    "\n",
    "# Initialize LightGBM classifier\n",
    "model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, learning_rate=0.05, n_estimators=100)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X(Thompsons)    Y(Counts) Y(Counts)_category\n",
      "0          40.0    46.235340                Low\n",
      "1          41.0    58.811787                Low\n",
      "2          41.0   617.287781             Medium\n",
      "3          42.0    62.857056                Low\n",
      "4          43.0  3057.952637               High\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 64617, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score -2.039175\n",
      "[LightGBM] [Info] Start training from score -0.557073\n",
      "[LightGBM] [Info] Start training from score -1.214088\n",
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "# Convert Y(Counts) into categories\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "bins = [0, 100, 1000, float('inf')]  # Define ranges for bins (adjust as needed)\n",
    "labels = ['Low', 'Medium', 'High']  # Define labels for bins\n",
    "combined_CM1_data['Y(Counts)_category'] = pd.cut(combined_CM1_data['Y(Counts)'], bins=bins, labels=labels)\n",
    "print(combined_CM1_data.head())\n",
    "\n",
    "# Use the new categorical target for classification\n",
    "X = combined_CM1_data[['X(Thompsons)']]  # Feature: X(Thompsons)\n",
    "y = combined_CM1_data['Y(Counts)_category']  # Target: Y(Counts) as categories\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n",
    "\n",
    "# Initialize LightGBM Classifier\n",
    "model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, learning_rate=0.05, n_estimators=100)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WOrking on CM2 dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WOrking on CM3 dataset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WOrking on CM1,CM2,CM3 combined\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
